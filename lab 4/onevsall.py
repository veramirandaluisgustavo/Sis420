# -*- coding: utf-8 -*-
"""onevsall.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18FFMeKlyWAlRilD7B6aneqrVYl3Aq1hu
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from google.colab import drive
import re
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from keras.datasets import mnist
from keras.models import Model
from keras.layers import Input, Flatten, Dense, Dropout,Flatten, Lambda
from keras.layers import Conv2D, Activation,AveragePooling2D,MaxPooling2D
from keras.optimizers import RMSprop
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import OneHotEncoder

drive.mount('/content/drive')

dataframe = pd.read_csv("/content/drive/MyDrive/datasets/94_character_TMNIST.csv")

dataframe.head(5)

all = list(dataframe['labels'].unique())
dataframe['labels'].unique()

capital_letters_regex = re.compile(r"[A-Z]")
small_letters_regex = re.compile(r"[a-z]")
numbers_regex = re.compile(r"[0-9]")
special_characters_regex = re.compile(r"[\W]|[\_\,]")

capital_letters = capital_letters_regex.findall(str(all))
small_letters = small_letters_regex.findall(str(all))
numbers = numbers_regex.findall(str(all))
special_characters = special_characters_regex.findall(str(all))
# Removing the empty space character
special_characters.pop(27)

fig, ax = plt.subplots()

characters = ["Letras Mayusculas", "Letras minusculas", "Numeros", "caracteres especiales"]

counts = [len(capital_letters), len(small_letters), len(numbers), len(special_characters)]

bar = ax.bar(characters, counts)

ax.bar_label(bar)


ax.set_ylabel('Total')
ax.set_xlabel('Tipos de caracter')
ax.set_title('Numero total de carateres')

plt.show()

#definir la columna de clase y eliminar otras columnas innecesarias
y=dataframe['labels']
X=dataframe.drop(['names','labels'],axis=1)

X_train, X_test, Y_train, Y_test = train_test_split( X, y, test_size=0.25, random_state=42,stratify=y)

plt.figure(figsize=(15, 6))
for i in range(15):
  plt.subplot(3, 5, i+1)
  plt.title(Y_train.iloc[i])
  # Convertir una matriz 1d a 2d usando remodelar
  plt.imshow(X_train.values[i].reshape(28,28), cmap=plt.get_cmap('gray'))

plt.show()

plt.figure(figsize=(30, 10))
plt.bar(Y_train.unique(),Y_train.value_counts(),color='#08B2E3')
plt.title('Distribución de clases en datos de entrenamiento')
plt.xlabel('Clases')
plt.ylabel('Contador')

plt.figure(figsize=(30, 10))
plt.bar(Y_test.unique(),Y_test.value_counts(),color='#653B2D')
plt.title('Distribución de clases en datos de prueba')
plt.xlabel('Clases')
plt.ylabel('Contador')

# Normalizando los datos
X_train= (X_train.astype('float32'))/255.0
X_test = (X_test.astype('float32'))/255.0

#codificar los valores y usando un codificador en caliente
enc = OneHotEncoder(sparse=False,handle_unknown='ignore')
y_train_encoded=enc.fit_transform(Y_train.values.reshape(-1,1))
y_test_encoded=  enc.transform(Y_test.values.reshape(-1,1))

X_train_norm=X_train.values.reshape(X_train.shape[0],28,28)
X_test_norm=X_test.values.reshape(X_test.shape[0],28,28)

# output_layer = dataframe['labels'].nunique()
# output_layer

output_layer = dataframe['labels'].nunique()
model = tf.keras.models.Sequential()
# Capa de entrada
model.add(Flatten(input_shape=(28,28)))
# Agregar 3 capas ocultas con longitud 512, 128 y 32 respectivamente
model.add(Dense(512,activation='relu'))
model.add(Dense(128,activation='relu'))
model.add(Dense(32, activation='relu'))
# Capa de salida. Ya que hay 94 caracteres.
model.add(Dense(94, activation='softmax'))
# opt = Adam()
model.compile(optimizer='adam', loss='mse', metrics=['accuracy', 'mse'])
model.summary()

#ajustando el modelo
model_history=model.fit(X_train_norm, y_train_encoded, epochs=20, validation_data=(X_test_norm, y_test_encoded), verbose=2,batch_size=128)

plt.figure(figsize=(14, 6))
plt.subplot(1,2,1)
epochs=np.arange(20)
plt.title('Exactitud vs ciclos')
plt.plot(epochs,model_history.history['accuracy'],label='train', color='#851D2D')
plt.ylabel('Exactitud')
plt.plot(epochs,model_history.history['val_accuracy'],label='test', color='#306844')
plt.legend()
plt.subplot(1,2,2)
plt.title('Perdidos vs Ciclos')
plt.plot(epochs,model_history.history['loss'],label='train', color='#851D2D')
plt.ylabel('Perdidos')
plt.plot(epochs,model_history.history['val_loss'],label='test', color='#306844')
plt.legend()
plt.show()

conv_model = tf.keras.models.Sequential()

conv_model.add(Conv2D(128, (5, 5), activation='relu', input_shape=(28, 28,1)))
conv_model.add(MaxPooling2D((2, 2)))

conv_model.add(Conv2D(64, (3, 3), activation='relu'))
conv_model.add(MaxPooling2D((2, 2)))

conv_model.add(Flatten())


conv_model.add(Dense(64, activation='relu'))
conv_model.add(Dense(32, activation='relu'))
conv_model.add(Dense(94, activation='softmax'))

# opt = Adam()
conv_model.compile(optimizer="adam", loss='mse', metrics=['accuracy'])
conv_model.summary()

X_train_norm=X_train_norm.reshape((X_train_norm.shape[0],28,28,1))
X_test_norm=X_test_norm.reshape((X_test_norm.shape[0],28,28,1))

#ajustando el modelo no lo entrenen porque se tarda dos horas
conv_history=conv_model.fit(X_train_norm, y_train_encoded, epochs=20, validation_data=(X_test_norm, y_test_encoded), verbose=2,batch_size=128)

plt.figure(figsize=(14, 6))
plt.subplot(1,2,1)

plt.title('Exactitud vs ciclos')
plt.plot(epochs,conv_history.history['accuracy'],label='train', color='#851D2D')
plt.ylabel('Exactitud')
plt.plot(epochs,conv_history.history['val_accuracy'],label='test', color='#306844')
plt.legend()
plt.subplot(1,2,2)
plt.title('Perdidos vs Ciclos')
plt.plot(epochs,conv_history.history['loss'],label='train', color='#851D2D')
plt.ylabel('Perdidos')
plt.plot(epochs,conv_history.history['val_loss'],label='test', color='#306844')
plt.legend()
plt.show()